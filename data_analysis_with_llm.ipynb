{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e239ab0",
   "metadata": {},
   "source": [
    "# Setting up LLM with Ollama for Data Analysis Agent\n",
    "\n",
    "This notebook sets up a Large Language Model (LLM) using llama3.1:70b via Ollama, and creates an agent with tools for data analysis, including a Python interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c5403",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Ollama installed and running locally.\n",
    "- The model `llama3.1:70b` pulled: Run `ollama pull llama3.1:70b` in your terminal.\n",
    "- Python packages installed (already done in this environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051412be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install docker requests langchain langchain-community langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph langchain-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1405ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"http://localhost:8765/plots/sales_by_region.png\"\n",
    "try:\n",
    "    r = requests.get(url)\n",
    "    print(f\"Status Code: {r.status_code}\")\n",
    "    print(f\"Content Type: {r.headers.get('Content-Type')}\")\n",
    "    print(f\"Content Length: {len(r.content)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bbcb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "fastmcp<3\n",
    "pandas>=2.2\n",
    "numpy>=1.26\n",
    "matplotlib>=3.9\n",
    "starlette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ff28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.12-slim\n",
    "\n",
    "# Create a non-root user\n",
    "RUN useradd -m sandboxuser\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY server.py /app/server.py\n",
    "\n",
    "# Create directories and set permissions\n",
    "RUN mkdir -p /app/data /app/plots && \\\n",
    "    chown -R sandboxuser:sandboxuser /app/data /app/plots\n",
    "\n",
    "# Switch to non-root user\n",
    "USER sandboxuser\n",
    "\n",
    "EXPOSE 8765\n",
    "\n",
    "CMD [\"python\", \"server.py\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile server.py\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "from contextlib import redirect_stdout\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from fastmcp import FastMCP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = \"/app/data\"\n",
    "PLOTS_DIR = \"/app/plots\"\n",
    "\n",
    "mcp = FastMCP(\n",
    "    name=\"Data Analyst Sandbox\",\n",
    "    instructions=\"Secure Python execution environment for data analysis\",\n",
    "    version=\"2025.12\"\n",
    ")\n",
    "\n",
    "@mcp.tool\n",
    "def execute_python_code(code: str) -> dict:\n",
    "    \"\"\"Execute Python code in a restricted environment\"\"\"\n",
    "    # Allow some safe builtins\n",
    "    safe_builtins = {\n",
    "        \"len\": len,\n",
    "        \"range\": range,\n",
    "        \"print\": print,\n",
    "        \"int\": int,\n",
    "        \"float\": float,\n",
    "        \"str\": str,\n",
    "        \"list\": list,\n",
    "        \"dict\": dict,\n",
    "        \"set\": set,\n",
    "        \"tuple\": tuple,\n",
    "        \"enumerate\": enumerate,\n",
    "        \"zip\": zip,\n",
    "        \"min\": min,\n",
    "        \"max\": max,\n",
    "        \"sum\": sum,\n",
    "    }\n",
    "    \n",
    "    namespace: Dict[str, Any] = {\n",
    "        \"pd\": pd,\n",
    "        \"np\": np,\n",
    "        \"plt\": plt,\n",
    "        \"os\": os,\n",
    "        \"DATA_DIR\": DATA_DIR,\n",
    "        \"PLOTS_DIR\": PLOTS_DIR,\n",
    "        \"__builtins__\": safe_builtins,\n",
    "    }\n",
    "\n",
    "    output = io.StringIO()\n",
    "    try:\n",
    "        # Clear previous plots to ensure we only return new ones\n",
    "        for f in os.listdir(PLOTS_DIR):\n",
    "            file_path = os.path.join(PLOTS_DIR, f)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        with redirect_stdout(output):\n",
    "            exec(code, namespace, namespace)\n",
    "\n",
    "        # Find newly created plots\n",
    "        plots = [\n",
    "            f\"http://localhost:8765/plots/{f}\"\n",
    "            for f in os.listdir(PLOTS_DIR)\n",
    "            if f.endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"output\": output.getvalue(),\n",
    "            \"plots\": plots,\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"output\": None,\n",
    "            \"error\": f\"{type(e).__name__}: {str(e)}\",\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "# Plot serving endpoint (using the underlying Starlette app)\n",
    "from starlette.responses import FileResponse, JSONResponse\n",
    "@mcp.http_app().route(\"/plots/{filename}\")\n",
    "async def serve_plot(request):\n",
    "    filename = request.path_params[\"filename\"]\n",
    "    file_path = os.path.join(PLOTS_DIR, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        return FileResponse(file_path)\n",
    "    return JSONResponse({\"error\": \"File not found\"}, status_code=404)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use HTTP transport to allow plot serving via the same port\n",
    "    mcp.run(transport=\"http\", host=\"0.0.0.0\", port=8765)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t mcp-data-sandbox:2025 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1247435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# First stop & remove if exists\n",
    "os.system(\"docker stop mcp-sandbox 2>nul\")\n",
    "os.system(\"docker rm mcp-sandbox 2>nul\")\n",
    "\n",
    "# Ensure local directories exist\n",
    "os.makedirs('local-data', exist_ok=True)\n",
    "os.makedirs('local-plots', exist_ok=True)\n",
    "\n",
    "# Run new container with volume mapping for both data and plots\n",
    "current_dir = os.getcwd()\n",
    "# Use absolute path for volume mapping\n",
    "docker_cmd = (\n",
    "    f'docker run -d --name mcp-sandbox -p 8765:8765 '\n",
    "    f'-v \"{current_dir}/local-data:/app/data\" '\n",
    "    f'-v \"{current_dir}/local-plots:/app/plots\" '\n",
    "    f'mcp-data-sandbox:2025'\n",
    ")\n",
    "print(f\"Running: {docker_cmd}\")\n",
    "os.system(docker_cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def upload_file_to_container(local_path: str):\n",
    "    \"\"\"Upload a local file to the Docker container's /app/data/ using docker cp.\"\"\"\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    \n",
    "    # Create local-data folder if needed (for volume, but docker cp doesn't require it)\n",
    "    os.makedirs('local-data', exist_ok=True)\n",
    "    \n",
    "    # Use docker cp to copy directly to container\n",
    "    container_path = \"mcp-sandbox:/app/data/\" + os.path.basename(local_path)\n",
    "    # Run docker command directly\n",
    "    subprocess.run([\"docker\", \"cp\", local_path, container_path], check=True)\n",
    "    \n",
    "    return f\"File {os.path.basename(local_path)} uploaded to container at /app/data/\"\n",
    "\n",
    "# Example usage (comment out or run as needed)\n",
    "upload_file_to_container('synthetic_sales_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210be871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mcp\n",
    "import mcp.types as mcp_types\n",
    "setattr(mcp, 'types', mcp_types)\n",
    "\n",
    "from langchain_core.tools import StructuredTool\n",
    "from pydantic import BaseModel, Field\n",
    "import asyncio\n",
    "from fastmcp import Client\n",
    "\n",
    "class ExecuteCodeSchema(BaseModel):\n",
    "    code: str = Field(..., description=\"Python code to execute in the sandbox\")\n",
    "\n",
    "async def call_mcp_execute_async(code: str) -> str:\n",
    "    \"\"\"Execute Python code in the MCP sandbox using the FastMCP Client.\"\"\"\n",
    "    try:\n",
    "        # Connect to the MCP server via HTTP (FastMCP v2 uses /mcp endpoint)\n",
    "        async with Client(\"http://localhost:8765/mcp\") as client:\n",
    "            # Call the tool\n",
    "            result = await client.call_tool(\"execute_python_code\", {\"code\": code})\n",
    "            \n",
    "            # Extract the text content from the result\n",
    "            if hasattr(result, 'content') and result.content:\n",
    "                return result.content[0].text\n",
    "            return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error calling MCP server: {str(e)}\"\n",
    "\n",
    "def call_mcp_execute(code: str) -> str:\n",
    "    \"\"\"Sync wrapper for the async MCP call.\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            return loop.run_until_complete(call_mcp_execute_async(code))\n",
    "        else:\n",
    "            return loop.run_until_complete(call_mcp_execute_async(code))\n",
    "    except Exception:\n",
    "        return asyncio.run(call_mcp_execute_async(code))\n",
    "\n",
    "mcp_execute = StructuredTool.from_function(\n",
    "    func=call_mcp_execute,\n",
    "    name=\"sandbox_python_exec\",\n",
    "    description=\"Execute Python code in the secure remote sandbox. Handles data analysis, plotting (save to /app/plots/), and returns output/plots URLs. Files are available in /app/data/ if uploaded manually.\",\n",
    "    args_schema=ExecuteCodeSchema\n",
    ")\n",
    "\n",
    "tools = [mcp_execute]\n",
    "print(\"MCP tool defined: sandbox_python_exec (using FastMCP Client SDK)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# LLM: Use llama3.1:70b for superior reasoning and robust tool calling\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    temperature=0.1,\n",
    "    num_predict=2048,\n",
    ")\n",
    "\n",
    "# Create the agent with revised tools\n",
    "agent_executor = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    prompt=(\n",
    "        \"You are a helpful data analyst. Use the sandbox_python_exec tool to perform analysis and create plots. \"\n",
    "        \"Always save plots to /app/plots/ and load data from /app/data/. \"\n",
    "        \"When you create a plot, the tool will return a URL. Mention this URL in your response so the user can see it.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Data Analyst Agent created with ReAct workflow (using llama3.1:70b).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e97d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import json\n",
    "\n",
    "def display_agent_response(response):\n",
    "    \"\"\"Helper to print agent messages and render any plots found in tool outputs.\"\"\"\n",
    "    for message in response['messages']:\n",
    "        if message.type == 'ai' and message.content:\n",
    "            print(f\"\\nAGENT: {message.content}\")\n",
    "        \n",
    "        if message.type == 'tool':\n",
    "            try:\n",
    "                # Parse the tool output (JSON string)\n",
    "                data = json.loads(message.content)\n",
    "                if isinstance(data, dict):\n",
    "                    # Print text output if present\n",
    "                    if data.get('output'):\n",
    "                        print(f\"\\n[Tool Output]:\\n{data['output']}\")\n",
    "                    \n",
    "                    # Display plots\n",
    "                    if data.get('plots'):\n",
    "                        for plot_url in data['plots']:\n",
    "                            print(f\"Rendering Plot: {plot_url}\")\n",
    "                            display(Image(url=plot_url))\n",
    "            except Exception as e:\n",
    "                # If it's not JSON, try to print raw or ignore\n",
    "                print(f\"\\n[Raw Tool Output]:\\n{message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd8aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Agent Test: Comprehensive Data Analysis\n",
    "import os\n",
    "\n",
    "# Define a complex question for the agent\n",
    "question = (\n",
    "    \"Analyze the sales data in 'synthetic_sales_data.csv'. \"\n",
    "    \"1. Calculate the total sales revenue (Quantity * Unit_Price). \"\n",
    "    \"2. Identify the top 2 products by revenue. \"\n",
    "    \"3. Create a bar chart showing total sales by Region.\"\n",
    "    \"4. Give a summary of findings. \"\n",
    "    \"Save the plot as '/app/plots/sales_by_region.png'.\"\n",
    ")\n",
    "\n",
    "print(f\"--- Final Agent Test ---\\nQuestion: {question}\\n\")\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent_executor.invoke({\"messages\": [(\"user\", question)]})\n",
    "\n",
    "# Display results and plots\n",
    "display_agent_response(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
